{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from hdx.utilities.easy_logging import setup_logging\n",
    "from hdx.hdx_configuration import Configuration\n",
    "from hdx.data.dataset import Dataset\n",
    "import pandas as pd\n",
    "from scipy import spatial\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from hdx.data.resource import Resource\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re \n",
    "import gensim\n",
    "import os;\n",
    "import re;\n",
    "import logging;\n",
    "import sqlite3;\n",
    "import sys;\n",
    "import multiprocessing;\n",
    "import matplotlib.pyplot as plt;\n",
    "from itertools import cycle;\n",
    "from io import StringIO\n",
    "import requests\n",
    "\n",
    "import Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawling, Extracting, Collecting, and Preprocessing HDX datasets\n",
    "<ol>\n",
    "<li> Access HDX API and search for specific set of datasets.</li>\n",
    "<li> Create a dataframe from the json of datasets. </li>\n",
    "<li> Extract resource objects for each dataset (if the dataset has any). </li>\n",
    "<li> Extract the header (column names) of the datsets if it has url (public url). </li>\n",
    "<li> Save content to a file for further processing. </li>\n",
    "<li> Eliminate stop words </li> \n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## add the 'geodata' to the dataset's metadata \n",
    "\n",
    "def add_geodata(df):\n",
    "    for i, row in df.iterrows():\n",
    "        if df.ix[i]['has_geodata'] == True:\n",
    "            df.at[i,'geodata'] = \"geodata\"\n",
    "    else:\n",
    "            df.at[i,'geodata'] = \" \"\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## add the country(ies) to the dataset's metadata\n",
    "\n",
    "def add_country(df):\n",
    "    for i, row in df.iterrows():\n",
    "    #print(row)\n",
    "        countryList=  df.ix[i]['solr_additions']\n",
    "        countries = \"\"\n",
    "        res = re.findall(r'\\w+', countryList)\n",
    "        for j in range(1,len(res)):\n",
    "            countries += \" \" + res[j]\n",
    "        df.at[i,'country'] =  countries.lower()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The main fields of the metadata that will be used for the processing\n",
    "#'notes' is a description on the dataset.\n",
    "def project_dataframe(df):\n",
    "    columns = ['title','notes','tags','organization','dataset_source','geodata', 'country']\n",
    "    df = df[columns]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract the header of the dataset (the columns)\n",
    "def extract_dataset_header(df):\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        try:\n",
    "            if df.ix[i]['id'] is not None and df.ix[i]['id'] != 0:\n",
    "                columns= extract_resource_header(df.ix[i]['id'])\n",
    "                df.at[i,'header'] = columns.lower()\n",
    "            else:\n",
    "                df.at[i,'header'] = \"\"\n",
    "        except:\n",
    "            #print(\"IO exception\")\n",
    "            continue\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the information of the organization that collect/share the dataset\n",
    "def extract_org_info(df):\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        tags = \" \"\n",
    "        organization = df.ix[i]['organization']\n",
    "        org = organization['description'] +\" \" + organization['title']\n",
    "        df.at[i,'organization'] = org.lower()\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# concatenate the content of all metadata fields into raw text\n",
    "def get_text(df):\n",
    "    #df.tags.astype(str)+ \" \" +\n",
    "    textdata =  df.title.fillna('').astype(str)+\" \" + df.header.fillna('').astype(str) +\" \" + df.organization.fillna('').astype(str) +\" \" +  df.notes.fillna('').astype(str)+\" \"+ df.country.fillna('').astype(str) + \" \" + df.geodata.fillna('').astype(str) \n",
    "    return textdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# concatenate the content of all metadata fields into raw text\n",
    "def get_all_text(df):\n",
    "    #df.tags.astype(str)+ \" \" +\n",
    "    textdata =  df.title.fillna('').astype(str)+\" \" + df.header.fillna('').astype(str) +\" \" + df.organization.fillna('').astype(str) +\" \" +  df.notes.fillna('').astype(str)+\" \"+ df.country.fillna('').astype(str) + \" \" + df.geodata.fillna('').astype(str) \n",
    "    return textdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## helper func.\n",
    "from gensim.utils import simple_preprocess\n",
    "def read_datasets(row):\n",
    "    #print(row)\n",
    "    return simple_preprocess(str(row).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper func.\n",
    "def print_word_table(table, key):\n",
    "    return pd.DataFrame(table, columns=[key, 'similarity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract the current tags of the dataset mainly for comparison and evaluation \n",
    "def extract_tags(df):\n",
    "    df['tag_list'] = [[] for _ in df.index]\n",
    "    for i, row in df.iterrows():\n",
    "        tags = \"\"\n",
    "        tag_list = []\n",
    "        for t in df.ix[i]['tags']:\n",
    "            #print(t)\n",
    "            #print( t['name'])\n",
    "            if t['name'] is not None:\n",
    "                tags+= t['name']+ \" \"\n",
    "                tag_list.append(t['name'])\n",
    "        if tags is None or tags.strip() ==\"\":\n",
    "            tags = \" \"\n",
    "        df.at[i,'tags'] = tags.lower()\n",
    "        df.at[i,'tag_list'] = tag_list\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def extract_resource_info(dataset):\n",
    " #   resource= dataset.get_resource()\n",
    "  #  return resource['url'], resource['created'].split('-')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract the resource objects for each dataset (if it has resource(s))\n",
    "def extract_resource(datasets):\n",
    "    dfObj = pd.DataFrame(columns=['id', 'package_id', 'url','date','format'])\n",
    "    for ds in datasets:\n",
    "        #print(ds)\n",
    "        if ds.is_requestable() == False:\n",
    "           #obj= None\n",
    "            obj = ds.get_resource(index=0)\n",
    "            #print(obj['format'])\n",
    "            dfObj = dfObj.append({'id': obj['id'], 'package_id':obj['package_id'],'url': obj['url'], 'date':obj['created'].split('-')[0],'format':obj['format'] },ignore_index=True)\n",
    "    return dfObj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read the xls and/or csv of using the url of the resources associated with the dataset\n",
    "def extract_resource_header(ds_id):\n",
    "    resource = Resource.read_from_hdx(str(ds_id))\n",
    "    if resource is not None:\n",
    "        header = \"\"\n",
    "\n",
    "        if resource['format'] == 'CSV':\n",
    "            #print(resource['url'])\n",
    "            #print(\"csv\" ,resource['url'])\n",
    "            s=requests.get(resource['url']).text\n",
    "            c=pd.read_csv(StringIO(s), nrows=2)#, header=None, error_bad_lines=False)\n",
    "            col_name = list(c.columns.values)\n",
    "            header = ' '.join(str(e) for e in col_name)\n",
    "        elif resource['format'] == 'XLS' or resource['format'] == 'XLSX':\n",
    "            c=pd.read_excel(resource['url'])#, sheetname=0, header=1)#StringIO(s)) #read_excel(BytesIO(s))#,header=None) #read_excel(StringIO(s))\n",
    "            col_name = list(c.columns.values)\n",
    "            header = ' '.join(str(e) for e in col_name)\n",
    "        else:\n",
    "            header = \"\"\n",
    "       # except IOError:\n",
    "         #   print(\"IO exception\")\n",
    "          #  continue\n",
    "            #pass\n",
    "            \n",
    "    else:\n",
    "        header = \"\"\n",
    "    return header\n",
    "    #return resource['url'], resource['created'].split('-')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://data.humdata.org/'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Connecting to HDX API\n",
    "Configuration.create(hdx_site='prod', user_agent='Crawling - Education datasets', hdx_read_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "extract_resource_header('cf0e8c85-e365-40f6-a7ab-58e98ad31e46')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cleaned_dataset=pd.read_csv(\"after_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This code snippets:\n",
    " 1) Access HDX API and request all the datasets that have tag = 'education'.\n",
    " 2) create a dataframe from the json of datasets\n",
    " 3) extract resource objects for each dataset (if the dataset has any).\n",
    " 4) use 'id', 'package_id' to join the resource object with the the dataset object.\n",
    " 5) extract the header (column names) of the datsets if it has url (public url)\n",
    " 6) save content to a file for further processing\n",
    "'''\n",
    "datasets = Dataset.search_in_hdx(fq='tags:education', rows=100000)\n",
    "df = pd.DataFrame.from_dict(datasets, orient='columns')\n",
    "df_resource= extract_resource(datasets)\n",
    "df.rename(columns={'id':'package_id'}, inplace=True)\n",
    "df = pd.merge(df, df_resource, on='package_id', how='outer')\n",
    "df['id'].fillna(0, inplace=True)\n",
    "df = extract_dataset_header(df)\n",
    "df.to_csv(\"metadat_before_cleaning.csv\", index=False)\n",
    "\n",
    "\n",
    "## Or load from prevoiusly prepared file \n",
    "#df_before = pd.read_csv(\"metadat_before_cleaning.csv\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This code snippets:\n",
    " 1) add geodata\n",
    " 2) add country\n",
    " 3) extract org_info\n",
    " 4) extract tags\n",
    " 5) clean the content (number, special character,links and/or non-english chars removal )\n",
    " 6) concatenate the text content from all metdata fields.\n",
    "'''\n",
    "\n",
    "columns = ['id','header','title','tags','notes','total_res_downloads','organization','dataset_source','geodata', 'country']#,'location']\n",
    "Pcolumns = ['title','header','tags','notes','organization','dataset_source','geodata', 'country']#,'location']\n",
    "df = add_geodata(df)\n",
    "df = add_country(df)\n",
    "df_process = df[['title','notes','tags','header','organization','dataset_source','geodata', 'country']] #'tags'\n",
    "df_process = extract_org_info(df_process)\n",
    "df_process=  extract_tags(df_process)\n",
    "df_process = Utility.data_clean(df_process,Pcolumns)\n",
    "df_process['doc']  =  get_text(df_process)\n",
    "df_process['All_text'] = get_all_text(df_process)\n",
    "#text_data = df_process['doc'] #get_text(df_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_process.ix[0]['doc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [education, health, hxl, population, transport]\n",
       "1      [education, education facilities, indonesia, p...\n",
       "2                                        [4w, education]\n",
       "3                                            [education]\n",
       "4      [dominica, education, openstreetmap, osm, scho...\n",
       "5             [education, palestine, schools, west bank]\n",
       "6            [education, gaza strip, palestine, schools]\n",
       "7      [directorate, districts, education, gaza strip...\n",
       "8      [education, education in emergencies, out of s...\n",
       "9               [conflict, education, gender, hxl, sadd]\n",
       "10     [education, openstreetmap, osm, schools, trini...\n",
       "11     [antigua and barbuda, education, openstreetmap...\n",
       "12     [barbados, education, openstreetmap, osm, scho...\n",
       "13                                           [education]\n",
       "14                                           [education]\n",
       "15                                           [education]\n",
       "16                                           [education]\n",
       "17                                           [education]\n",
       "18                                           [education]\n",
       "19     [acu, aleppo, camps, education, idleb, idps, i...\n",
       "20     [cholera, diarrhea, disease, displacement, dro...\n",
       "21                                           [education]\n",
       "22                                           [education]\n",
       "23                                           [education]\n",
       "24                                           [education]\n",
       "25                                           [education]\n",
       "26                                           [education]\n",
       "27                                           [education]\n",
       "28                                           [education]\n",
       "29                                           [education]\n",
       "                             ...                        \n",
       "578    [data lab nairobi, education, primary school e...\n",
       "579            [data lab nairobi, education, population]\n",
       "580                                 [education, schools]\n",
       "581                       [burkina faso, education, hxl]\n",
       "582              [economy, education, hdi, health, sadd]\n",
       "583    [africa, demographics, education, gdp, health,...\n",
       "584              [economy, education, hdi, health, sadd]\n",
       "585              [economy, education, hdi, health, sadd]\n",
       "586              [economy, education, hdi, health, sadd]\n",
       "587    [african countries, education, gender parity i...\n",
       "588                                    [education, sadd]\n",
       "589    [african countries, education, gender parity i...\n",
       "590              [economy, education, hdi, health, sadd]\n",
       "591                    [economy, education, hdi, health]\n",
       "592                    [economy, education, hdi, health]\n",
       "593                    [economy, education, hdi, health]\n",
       "594    [3w, education, food security, health, nutriti...\n",
       "595      [assessment, damage, education, libya, schools]\n",
       "596                                  [census, education]\n",
       "597                        [education, enrollment ratio]\n",
       "598       [data lab nairobi, education, learning levels]\n",
       "599    [african countries, education, gross enrolment...\n",
       "600    [3w, camp coordination and management, educati...\n",
       "601              [data lab nairobi, education, literacy]\n",
       "602               [education, geodata, nepal earthquake]\n",
       "603              [data lab nairobi, education, learning]\n",
       "604                               [3w, ebola, education]\n",
       "605                        [education, nepal earthquake]\n",
       "606    [data lab nairobi, education, food and nutriti...\n",
       "607    [census, demographics, education, geodata, hea...\n",
       "Name: tag_list, Length: 608, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the tag_list of each dataset.\n",
    "df_process['tag_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words= set(stopwords.words('english'))\n",
    "stop_words.update(['unnamed','nan','file','xls','xlsx','zip','link', 'description','https'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save content after cleaning\n",
    "df_process.to_csv(\"after_clean.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_process.to_csv(\"after_clean_v3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## stop words removal.\n",
    "df_process['doc'] = Utility.remove_stopwords(df_process['doc'], stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#text_data =remove_stopwords(df_process['doc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#all_text_data =remove_stopwords(df_process['All_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_process['All_text'] =remove_stopwords(df_process['All_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## save preprocessed content into file\n",
    "df_process.to_csv(\"after_preprocessing.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'somalia demographic health education transport indicators category indicator indicator friendly type data latitude longitude region id country id name year value global urban observatory helps cities bird view situation needs using geographical information systems technology helps cities bird view situation needs photograph city space magnify look streets area send survey teams fill blanks streets many people live many access water sanitation roads need repair many people aids malaria slums overcrowded armed answers questions easier cheaper bring improvements information gathered researched provides database statistics indicators state urban development around world information provided enables habitat monitor implementation target millennium development goals significantly improve lives million slum dwellers year united nations human settlement programmes global urban observatory urban indicators data available analyzed compiled published habitat global urban observatory supports governments local authorities civil society organizations develop urban indicators data statistics urban statistics collected household surveys censuses conducted national statistics authorities global urban observatory team analyses compiles urban indicators statistics surveys censuses additionally local urban observatories collect compile analyze urban data national policy development population statistics produced united nations department economic social affairs world urbanization prospects somalia'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_process.ix[0]['doc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
